{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6y9fpRi4jeJl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651855863228,"user_tz":-540,"elapsed":34781,"user":{"displayName":"Weijian Liu","userId":"08903921306987121215"}},"outputId":"c60186a0-33ab-4fcb-fa62-0914fe2ac883"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyLDAvis\n","  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n","\u001b[?25l\r\u001b[K     |▏                               | 10 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 112 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 122 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 133 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 153 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 163 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 174 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 184 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 194 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 204 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 215 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 225 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 235 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 245 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 256 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 266 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 276 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 286 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 296 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 307 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 317 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 327 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 337 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 348 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 358 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 368 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 378 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 389 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 399 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 409 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 419 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 430 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 440 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 450 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 460 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 471 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 481 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 491 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 501 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 512 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 522 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 532 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 542 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 552 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 563 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 573 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 583 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 593 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 604 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 614 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 624 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 634 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 645 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 655 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 665 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 675 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 686 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 696 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 706 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 716 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 727 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 737 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 747 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 757 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 768 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 778 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 788 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 798 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 808 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 819 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 829 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 839 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 849 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 860 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 870 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 880 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 890 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 901 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 911 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 921 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 931 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 942 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 952 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 962 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 972 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 983 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 993 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.0 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.0 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.0 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.0 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.3 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.3 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.3 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.3 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7 MB 9.2 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n","Collecting funcy\n","  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.1)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (6.0.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis) (3.0.8)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n","Building wheels for collected packages: pyLDAvis\n","  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=47e7d08169f113326bfda3904ccef74c254f16315324c32976cecc02539f5e52\n","  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n","Successfully built pyLDAvis\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-1.17 pyLDAvis-3.3.1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","  from collections import Iterable\n","/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n","  from collections import Mapping\n","/usr/local/lib/python3.7/dist-packages/scipy/fft/__init__.py:97: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n","  from numpy.dual import register_func\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n","  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n","/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n","/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n"]}],"source":["!pip install pyLDAvis\n","import pyLDAvis\n","import pyLDAvis.gensim_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JppQHFqdaKfS"},"outputs":[],"source":["import nltk\n","nltk.download('all')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbaBUB6ajJV0"},"outputs":[],"source":["import pandas as pd\n","from nltk.corpus import wordnet\n","import string\n","from nltk import pos_tag\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import gensim\n","import gensim.corpora as corpora\n","import re\n","import os\n","import seaborn as sns\n","from gensim.models.coherencemodel import CoherenceModel\n","import numpy as np\n","import glob"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"pmTY5W1Wj3JA","executionInfo":{"status":"ok","timestamp":1651855960381,"user_tz":-540,"elapsed":363,"user":{"displayName":"Weijian Liu","userId":"08903921306987121215"}}},"outputs":[],"source":["#create functions\n","def get_wordnet_pos(pos_tag):\n","    if pos_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif pos_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif pos_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif pos_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.ADV\n","    \n","\n","def clean_text(text):\n","    # lower text\n","    text = text.lower()\n","    # tokenize text and remove puncutation\n","    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n","    # remove words that contain numbers\n","    text = [word for word in text if not any(c.isdigit() for c in word)]\n","    # remove stop words\n","    stop = stopwords.words('english')\n","    text = [x for x in text if x not in stop]\n","    # remove empty tokens\n","    text = [t for t in text if len(t) > 0]\n","    # pos tag text\n","    pos_tags = pos_tag(text)\n","    # lemmatize text\n","    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags if get_wordnet_pos(t[1]) == wordnet.NOUN]\n","    # remove words with only one letter\n","    text = [t for t in text if len(t) > 1]\n","    # join all\n","    text = \" \".join(text)\n","    return(text)\n","\n","def gen_words(texts):\n","    final = []\n","    for text in texts:\n","        new = gensim.utils.simple_preprocess(text, deacc=True)\n","        final.append(new)\n","    return final\n","\n","def make_corpus(data_words):\n","    corpus = []\n","    for text in data_words:\n","        id2word = corpora.Dictionary(data_words)\n","        new = id2word.doc2bow(text)\n","        corpus.append(new)\n","    return corpus\n","\n","\n","def LDA(lst, file_name, ntopics):\n","    file_name = file_name\n","    lemmatized_texts = lst\n","    data_words = gen_words(lemmatized_texts)\n","    corpus = make_corpus(data_words)\n","    id2word = corpora.Dictionary(data_words)\n","    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                               id2word=id2word,\n","                                               num_topics=ntopics,\n","                                               random_state=0,\n","                                               update_every=1,\n","                                               chunksize=100,\n","                                               passes=10,\n","                                               alpha=\"auto\")\n","    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\")\n","    pyLDAvis.save_html(vis, file_name+ \".html\")\n","    print(\"[0]: lda_model\\n[1]: corpus\\n[2]: id2word\")\n","    return (lda_model, corpus, id2word)\n","\n","def LDAnoVis(lst, ntopics):\n","    lemmatized_texts = lst\n","    data_words = gen_words(lemmatized_texts)\n","    corpus = make_corpus(data_words)\n","    id2word = corpora.Dictionary(data_words)\n","    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                               id2word=id2word,\n","                                               num_topics=ntopics,\n","                                               random_state=0,\n","                                               update_every=1,\n","                                               chunksize=100,\n","                                               passes=10,\n","                                               alpha=\"auto\")\n","    return (lda_model, corpus, id2word)\n","\n","\n","def CoherenceLDA(data, iteration):\n","    Colst = []\n","    times = 0\n","    for i in range(1, iteration + 1):\n","        res = LDAnoVis(data, i)\n","        cm = CoherenceModel(\n","            model=res[0],\n","            corpus=res[1],\n","            dictionary=res[2],\n","            coherence='u_mass', # Coherenceの算出方法を指定。 (デフォルトは'c_v')\n","            topn=20 # 各トピックの上位何単語から算出するか指定(デフォルト20)\n","            )\n","        Colst.append(cm.get_coherence())\n","        times = times + 1\n","        print(str(times) + \"/\" + str(iteration))\n","    \n","    df = pd.DataFrame()\n","    df[\"N_of_Topic\"] = list(range(1, iteration + 1))\n","    df[\"Coherence\"] = Colst\n","    df[\"min_max_Coh\"] = (df[\"Coherence\"] - df[\"Coherence\"].min()) / (df[\"Coherence\"].max() - df[\"Coherence\"].min())\n","    return df\n","\n","def PerplexityLDA(data, iteration):\n","    Perlst = []\n","    times = 0\n","    for i in range(1, iteration + 1):\n","        res = LDAnoVis(data, i)\n","        per = np.exp(-res[0].log_perplexity(res[1]))\n","        Perlst.append(per)\n","        times = times + 1\n","        print(str(times) + \"/\" + str(iteration))\n","    \n","    df = pd.DataFrame()\n","    df[\"N_of_Topic\"] = list(range(1, iteration + 1))\n","    df[\"Perplexity\"] = Perlst\n","    df[\"min_max_Per\"] = (df[\"Perplexity\"] - df[\"Perplexity\"].min()) / (df[\"Perplexity\"].max() - df[\"Perplexity\"].min())\n","    return df\n","\n","def gen_dfwords(lda, ntopics):\n","    twords = []\n","    rows = []\n","    for i in range(0,ntopics):\n","        rows.append(\"Topic\" + str(i+1))\n","        twords.append([ word.split('\"')[1] for word in lda.print_topic(i).split('+') ])\n","   \n","    dfwords = pd.DataFrame(twords, index=rows)\n","    return dfwords\n","    \n","\n","def gen_dfByLDA(lda, tcorpus, ntopics):\n","  #get the value of topics in each documents\n","    topics = []\n","    for c in tcorpus:\n","        topic = [0] * ntopics\n","        for (tpc, prob) in lda.get_document_topics(c):\n","            topic[tpc] = prob\n","        topics.append(topic)\n","  #get the highest value of topic in each documents\n","    clusters = []\n","    for x in topics:\n","        clusters.append(x.index(max(x)))\n","    #cluster means the highest value of topic in each documents\n","    return [topics, clusters]"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Ubh9jZ9YkR7c","executionInfo":{"status":"ok","timestamp":1651855963372,"user_tz":-540,"elapsed":255,"user":{"displayName":"Weijian Liu","userId":"08903921306987121215"}}},"outputs":[],"source":["#create a function so we can just input a textdata then we get all we want\n","#This function need a list of text input\n","def brieflyLDA_CohPer(folderpath, iteration):\n","  globpath = glob.glob(folderpath + \"/*.txt\")\n","  text_lst = []\n","  for txt in globpath:\n","    with open(txt) as f:\n","      text_lst.append(f.read())\n","\n","  text_lst_cleaned = [clean_text(i) for i in text_lst]\n","  df_Coh = CoherenceLDA(text_lst_cleaned, iteration)\n","  df_Per = PerplexityLDA(text_lst_cleaned, iteration)\n","  df = pd.merge(df_Coh, df_Per, left_index=True, right_index=True)\n","  df.to_excel(folderpath + \".xlsx\", index=False)"]},{"cell_type":"code","source":["def brieflyLDA(folderpath, num_of_topics):\n","  globpath = glob.glob(folderpath + \"/*.txt\")\n","  text_lst = []\n","  for txt in globpath:\n","    with open(txt) as f:\n","      text_lst.append(f.read())\n","\n","  text_lst_cleaned = [clean_text(i) for i in text_lst]\n","  file_name = folderpath.replace(\"/content/\", \"\")\n","  LDAmodel = LDA(text_lst_cleaned, file_name, num_of_topics)\n","  article_name_lst = [(re.search(\"((?:[^/]*/)*)(.*)\", path)).groups()[1] for path in globpath]\n","\n","  Topic_of_doc = gen_dfByLDA(LDAmodel[0], LDAmodel[1], num_of_topics)\n","  df = pd.DataFrame({\"Article\": article_name_lst, \"Topic\": Topic_of_doc[1]})\n","  df.to_excel(file_name + \".xlsx\", index=False)\n","  return df"],"metadata":{"id":"r64hTaIjcB8c","executionInfo":{"status":"ok","timestamp":1651855966746,"user_tz":-540,"elapsed":247,"user":{"displayName":"Weijian Liu","userId":"08903921306987121215"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\"/content/before2011\".replace(\"/content/\", \"\") + \".xlsx\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"-8YARUomlFlp","executionInfo":{"status":"ok","timestamp":1651827350986,"user_tz":-540,"elapsed":263,"user":{"displayName":"Weijian Liu","userId":"08903921306987121215"}},"outputId":"12b1d770-41e0-4e59-dfbd-a47b916f9285"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'before2011.xlsx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gEz9o-MaVObt"},"outputs":[],"source":["!unzip \"/content/2012to2015.zip\"\n","!unzip \"/content/2016to2019.zip\"\n","!unzip \"/content/2020to2021.zip\"\n","!unzip \"/content/before2011.zip\""]},{"cell_type":"code","source":["!unzip \"/content/2016to2019.zip\"\n","!unzip \"/content/2020to2021.zip\""],"metadata":{"id":"ARhR8cuRjTcC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87LzK7TDZcbC"},"outputs":[],"source":["brieflyLDA_CohPer(\"/content/2012to2015\", 20)\n","brieflyLDA_CohPer(\"/content/2016to2019\", 20)\n","brieflyLDA_CohPer(\"/content/2020to2021\", 20)\n","brieflyLDA_CohPer(\"/content/before2011\", 20)"]},{"cell_type":"code","source":["brieflyLDA_CohPer(\"/content/2020to2021\", 20)"],"metadata":{"id":"CmKZ5qKreXyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LDAbefore2011 = brieflyLDA(\"/content/before2011\", 12)\n","LDA2012to2015 = brieflyLDA(\"/content/2012to2015\", 5)\n","LDA2016to2019 = brieflyLDA(\"/content/2016to2019\", 8)\n","LDA2020to2021 = brieflyLDA(\"/content/2020to2021\", 3)"],"metadata":{"id":"bsN41K9KiUPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LDA2020to2021 = brieflyLDA(\"/content/2020to2021\", 3)"],"metadata":{"id":"spwf6DE0nQv1"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Gensim_LDA.ipynb","provenance":[],"authorship_tag":"ABX9TyNmPs5Hvd5zL5MtllzOB1+J"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}